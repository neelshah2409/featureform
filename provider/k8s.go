package provider

import (
	"bytes"
	"encoding/json"
	"fmt"
	"os"
	"os/exec"
	"strconv"
	"strings"
	"time"

	"github.com/featureform/metadata"
	"github.com/parquet-go/parquet-go"

	dp "github.com/novln/docker-parser"
	"go.uber.org/zap"
	_ "gocloud.dev/blob/fileblob"
	_ "gocloud.dev/blob/memblob"
	"golang.org/x/exp/slices"

	cfg "github.com/featureform/config"
	filestore "github.com/featureform/filestore"
	"github.com/featureform/helpers"
	"github.com/featureform/kubernetes"
	"github.com/featureform/logging"
	pc "github.com/featureform/provider/provider_config"
)

type pandasOfflineQueries struct {
	defaultPythonOfflineQueries
}

func (q pandasOfflineQueries) trainingSetCreate(def TrainingSetDef, featureSchemas []ResourceSchema, labelSchema ResourceSchema) string {
	columns := make([]string, 0)
	joinQueries := make([]string, 0)
	featureTimestamps := make([]string, 0)
	for i, feature := range def.Features {
		featureColumnName := createQuotedIdentifier(feature)
		columns = append(columns, featureColumnName)
		var featureWindowQuery string
		// if no timestamp column, set to default generated by resource registration
		if featureSchemas[i].TS == "" {
			featureWindowQuery = fmt.Sprintf("SELECT * FROM (SELECT %s as t%d_entity, %s as %s, 0 as t%d_ts FROM source_%d) ORDER BY t%d_ts ASC", featureSchemas[i].Entity, i+1, featureSchemas[i].Value, featureColumnName, i+1, i+1, i+1)
		} else {
			featureWindowQuery = fmt.Sprintf("SELECT * FROM (SELECT %s as t%d_entity, %s as %s, %s as t%d_ts FROM source_%d) ORDER BY t%d_ts ASC", featureSchemas[i].Entity, i+1, featureSchemas[i].Value, featureColumnName, featureSchemas[i].TS, i+1, i+1, i+1)
		}
		featureJoinQuery := fmt.Sprintf("LEFT OUTER JOIN (%s) t%d ON (t%d_entity = entity AND t%d_ts <= label_ts)", featureWindowQuery, i+1, i+1, i+1)
		joinQueries = append(joinQueries, featureJoinQuery)
		featureTimestamps = append(featureTimestamps, fmt.Sprintf("t%d_ts", i+1))
	}
	for i, lagFeature := range def.LagFeatures {
		lagFeaturesOffset := len(def.Features)
		idx := slices.IndexFunc(def.Features, func(id ResourceID) bool {
			return id.Name == lagFeature.FeatureName && id.Variant == lagFeature.FeatureVariant
		})
		lagSource := fmt.Sprintf("source_%d", idx)
		lagColumnName := sanitize(lagFeature.LagName)
		if lagFeature.LagName == "" {
			lagColumnName = sanitize(fmt.Sprintf("%s_%s_lag_%s", lagFeature.FeatureName, lagFeature.FeatureVariant, lagFeature.LagDelta))
		}
		columns = append(columns, lagColumnName)
		timeDeltaSeconds := lagFeature.LagDelta.Seconds() //parquet stores time as microseconds
		curIdx := lagFeaturesOffset + i + 1
		var lagWindowQuery string
		if featureSchemas[idx].TS == "" {
			lagWindowQuery = fmt.Sprintf("SELECT * FROM (SELECT %s as t%d_entity, %s as %s, 0 as t%d_ts FROM %s) ORDER BY t%d_ts ASC", featureSchemas[idx].Entity, curIdx, featureSchemas[idx].Value, lagColumnName, curIdx, lagSource, curIdx)
		} else {
			lagWindowQuery = fmt.Sprintf("SELECT * FROM (SELECT %s as t%d_entity, %s as %s, %s as t%d_ts FROM %s) ORDER BY t%d_ts ASC", featureSchemas[idx].Entity, curIdx, featureSchemas[idx].Value, lagColumnName, featureSchemas[idx].TS, curIdx, lagSource, curIdx)
		}
		lagJoinQuery := fmt.Sprintf("LEFT OUTER JOIN (%s) t%d ON (t%d_entity = entity AND DATETIME(t%d_ts, '+%f seconds') <= label_ts)", lagWindowQuery, curIdx, curIdx, curIdx, timeDeltaSeconds)
		joinQueries = append(joinQueries, lagJoinQuery)
		featureTimestamps = append(featureTimestamps, fmt.Sprintf("t%d_ts", curIdx))
	}
	columnStr := strings.Join(columns, ", ")
	joinQueryString := strings.Join(joinQueries, " ")
	var labelWindowQuery string
	if labelSchema.TS == "" {
		labelWindowQuery = fmt.Sprintf("SELECT %s AS entity, %s AS value, 0 AS label_ts FROM source_0", labelSchema.Entity, labelSchema.Value)
	} else {
		labelWindowQuery = fmt.Sprintf("SELECT %s AS entity, %s AS value, %s AS label_ts FROM source_0", labelSchema.Entity, labelSchema.Value, labelSchema.TS)
	}
	labelPartitionQuery := fmt.Sprintf("(SELECT * FROM (SELECT entity, value, label_ts FROM (%s) t ) t0)", labelWindowQuery)
	labelJoinQuery := fmt.Sprintf("%s %s", labelPartitionQuery, joinQueryString)

	timeStamps := strings.Join(featureTimestamps, ", ")
	timeStampsDesc := strings.Join(featureTimestamps, " DESC,")
	fullQuery := fmt.Sprintf("SELECT %s, value AS %s, entity, label_ts, %s, ROW_NUMBER() over (PARTITION BY entity, value, label_ts ORDER BY label_ts DESC, %s DESC) as row_number FROM (%s) tt", columnStr, createQuotedIdentifier(def.Label), timeStamps, timeStampsDesc, labelJoinQuery)
	finalQuery := fmt.Sprintf("SELECT %s, %s FROM (SELECT * FROM (SELECT *, row_number FROM (%s) WHERE row_number=1 ))  ORDER BY label_ts", columnStr, createQuotedIdentifier(def.Label), fullQuery)
	return finalQuery
}

type K8sOfflineStore struct {
	executor Executor
	store    FileStore
	logger   *zap.SugaredLogger
	query    *pandasOfflineQueries
	BaseProvider
}

func (k8s *K8sOfflineStore) AsOfflineStore() (OfflineStore, error) {
	return k8s, nil
}

func (k8s *K8sOfflineStore) Close() error {
	return k8s.store.Close()
}

type Config []byte

type ExecutorFactory func(config Config, logger *zap.SugaredLogger) (Executor, error)

var executorFactoryMap = make(map[string]ExecutorFactory)

func RegisterExecutorFactory(name string, executorFactory ExecutorFactory) error {
	if _, exists := executorFactoryMap[name]; exists {
		return fmt.Errorf("factory already registered: %s", name)
	}
	executorFactoryMap[name] = executorFactory
	return nil
}

func CreateExecutor(name string, config Config, logger *zap.SugaredLogger) (Executor, error) {
	factory, exists := executorFactoryMap[name]
	if !exists {
		return nil, fmt.Errorf("factory does not exist: %s", name)
	}
	executor, err := factory(config, logger)
	if err != nil {
		return nil, err
	}
	return executor, nil
}

type FileStoreFactory func(config Config) (FileStore, error)

var fileStoreFactoryMap = make(map[string]FileStoreFactory)

func RegisterFileStoreFactory(name string, FileStoreFactory FileStoreFactory) error {
	if _, exists := fileStoreFactoryMap[name]; exists {
		return fmt.Errorf("factory already registered: %s", name)
	}
	fileStoreFactoryMap[name] = FileStoreFactory
	return nil
}

func CreateFileStore(name string, config Config) (FileStore, error) {
	factory, exists := fileStoreFactoryMap[name]
	if !exists {
		return nil, fmt.Errorf("factory does not exist: %s", name)
	}
	FileStore, err := factory(config)
	if err != nil {
		return nil, fmt.Errorf("failed to create FileStore: %v", err)
	}
	return FileStore, nil
}

func init() {
	FileStoreFactoryMap := map[filestore.FileStoreType]FileStoreFactory{
		filestore.FileSystem: NewLocalFileStore,
		filestore.Azure:      NewAzureFileStore,
		filestore.S3:         NewS3FileStore,
		filestore.GCS:        NewGCSFileStore,
		filestore.HDFS:       NewHDFSFileStore,
	}
	executorFactoryMap := map[pc.ExecutorType]ExecutorFactory{
		pc.GoProc: NewLocalExecutor,
		pc.K8s:    NewKubernetesExecutor,
	}
	for storeType, factory := range FileStoreFactoryMap {
		err := RegisterFileStoreFactory(string(storeType), factory)
		if err != nil {
			panic(err)
		}
	}
	for executorType, factory := range executorFactoryMap {
		err := RegisterExecutorFactory(string(executorType), factory)
		if err != nil {
			panic(err)
		}
	}
}

func k8sOfflineStoreFactory(config pc.SerializedConfig) (Provider, error) {
	k8 := pc.K8sConfig{}
	logger := logging.NewLogger("kubernetes")
	if err := k8.Deserialize(config); err != nil {
		logger.Errorw("Invalid config to initialize k8s offline store", "error", err)
		return nil, fmt.Errorf("invalid k8s config: %w", err)
	}
	logger.Info("Creating executor with type:", k8.ExecutorType)
	execConfig := k8.ExecutorConfig.(pc.ExecutorConfig)
	serializedExecutor, err := execConfig.Serialize()
	if err != nil {
		logger.Errorw("Failure serializing executor", "executor_type", k8.ExecutorType, "error", err)
		return nil, err
	}
	executor, err := CreateExecutor(string(k8.ExecutorType), serializedExecutor, logger)
	if err != nil {
		logger.Errorw("Failure initializing executor", "executor_type", k8.ExecutorType, "error", err)
		return nil, err
	}

	serializedBlob, err := k8.StoreConfig.Serialize()
	if err != nil {
		return nil, fmt.Errorf("could not serialize blob store config")
	}

	logger.Info("Creating blob store with type:", k8.StoreType)
	store, err := CreateFileStore(string(k8.StoreType), serializedBlob)
	if err != nil {
		logger.Errorw("Failure initializing blob store with type", k8.StoreType, err)
		return nil, err
	}
	logger.Debugf("Store type: %s", k8.StoreType)
	queries := pandasOfflineQueries{}
	k8sOfflineStore := K8sOfflineStore{
		executor: executor,
		store:    store,
		logger:   logger,
		query:    &queries,
		BaseProvider: BaseProvider{
			ProviderType:   "K8S_OFFLINE",
			ProviderConfig: config,
		},
	}
	return &k8sOfflineStore, nil
}

type Executor interface {
	ExecuteScript(envVars map[string]string, args *metadata.KubernetesArgs) error
}

type LocalExecutor struct {
	scriptPath string
}

func (local LocalExecutor) ExecuteScript(envVars map[string]string, args *metadata.KubernetesArgs) error {
	envVars["MODE"] = "local"
	for key, value := range envVars {
		if err := os.Setenv(key, value); err != nil {
			return fmt.Errorf("could not set env variable: %s: %w", key, err)
		}
	}
	cmd := exec.Command("python3", local.scriptPath)
	cmd.Stdout = os.Stdout
	cmd.Stderr = os.Stderr
	if err := cmd.Run(); err != nil {
		return fmt.Errorf("could not execute python function: %v", err)
	}
	return nil
}

type LocalExecutorConfig struct {
	ScriptPath string
}

func (config *LocalExecutorConfig) Serialize() ([]byte, error) {
	data, err := json.Marshal(config)
	if err != nil {
		return nil, err
	}
	return data, nil
}

func (config *LocalExecutorConfig) Deserialize(data []byte) error {
	err := json.Unmarshal(data, config)
	if err != nil {
		return fmt.Errorf("deserialize executor config: %w", err)
	}
	return nil
}

func NewLocalExecutor(config Config, logger *zap.SugaredLogger) (Executor, error) {
	localConfig := LocalExecutorConfig{}
	if err := localConfig.Deserialize(config); err != nil {
		return nil, fmt.Errorf("failed to deserialize config")
	}
	_, err := os.Open(localConfig.ScriptPath)
	if err != nil {
		return nil, fmt.Errorf("could not find script path: %v", err)
	}
	return LocalExecutor{
		scriptPath: localConfig.ScriptPath,
	}, nil
}

type KubernetesExecutor struct {
	logger *zap.SugaredLogger
	image  string
}

// isDefaultImage checks that the current image name (excluding the tag) is the same as the default image
// name config.PandasBaseImage. It also validates that the name is a valid docker image name
func (kube *KubernetesExecutor) isDefaultImage() (bool, error) {
	parse, err := dp.Parse(kube.image)
	if err != nil {
		return false, fmt.Errorf("invalid image name: %w", err)
	}
	return parse.ShortName() == cfg.PandasBaseImage, nil
}

func (kube *KubernetesExecutor) setCustomImage(image string) {
	if image != "" {
		kube.image = image
	}
}

func (kube *KubernetesExecutor) ExecuteScript(envVars map[string]string, args *metadata.KubernetesArgs) error {
	kube.logger.Debugw("Executing k8s script", "args", args)
	var specs metadata.KubernetesResourceSpecs
	if args != nil {
		kube.setCustomImage(args.DockerImage)
		specs = args.Specs
	}
	if isDefault, err := kube.isDefaultImage(); err != nil {
		return fmt.Errorf("image check failed: %w", err)
	} else if !isDefault {
		kube.logger.Warnf("You are using a custom Docker Image (%s) for a Kubernetes job. This may have unintended behavior.", kube.image)
	}
	envVars["MODE"] = "k8s"
	resourceType, err := strconv.Atoi(envVars["RESOURCE_TYPE"])
	if err != nil {
		resourceType = 0
	}
	config := kubernetes.KubernetesRunnerConfig{
		JobPrefix: "kcf",
		EnvVars:   envVars,
		Image:     kube.image,
		NumTasks:  1,
		Resource: metadata.ResourceID{
			Name:    envVars["RESOURCE_NAME"],
			Variant: envVars["RESOURCE_VARIANT"],
			Type:    ProviderToMetadataResourceType[OfflineResourceType(resourceType)],
		},
		Specs: specs,
	}
	jobRunner, err := kubernetes.NewKubernetesRunner(config)
	if err != nil {
		return err
	}
	completionWatcher, err := jobRunner.Run()
	if err != nil {
		return err
	}
	if err := completionWatcher.Wait(); err != nil {
		return err
	}
	return nil
}

func NewKubernetesExecutor(config Config, logger *zap.SugaredLogger) (Executor, error) {
	var c pc.ExecutorConfig
	err := c.Deserialize(config)
	if err != nil {
		return nil, fmt.Errorf("could not create Kubernetes Executor: %w", err)
	}
	return &KubernetesExecutor{
		image:  c.GetImage(),
		logger: logger,
	}, nil
}

func ResourcePrefix(id ResourceID) string {
	return fmt.Sprintf("featureform/%s/%s/%s", id.Type, id.Name, id.Variant)
}

// TODO: remove all remaining references to this function in favor of
// ResourceID.FilestorePath
func fileStoreResourcePath(id ResourceID) string {
	return ResourcePrefix(id)
}

type BlobOfflineTable struct {
	store  FileStore
	schema ResourceSchema
}

// TODO: implement
func (tbl *BlobOfflineTable) Write(ResourceRecord) error {
	return fmt.Errorf("not yet implemented")
}

func (tbl *BlobOfflineTable) WriteBatch(records []ResourceRecord) error {
	destination, err := filestore.NewEmptyFilepath(tbl.store.FilestoreType())
	if err != nil {
		return fmt.Errorf("could not create file path: %w", err)
	}
	if err := destination.ParseFilePath(tbl.schema.SourceTable); err != nil {
		return fmt.Errorf("could not parse file path: %w", err)
	}
	exists, err := tbl.store.Exists(destination)
	if err != nil {
		return fmt.Errorf("could not check if destination file exists: %w", err)
	}
	// It's very possible the resource table's data source already exists, in which case we'll
	// need to read the existing data and append the new records to it.
	if exists {
		// **NOTE:**Consider a better way of handling records written to resource tables such that
		// we don't rely on the order of rows in the file for materializations; currently, we're
		// counting on the implicit ordering of rows once these files are read into Spark and
		// queried for materializations.
		iter, err := tbl.store.Serve([]filestore.Filepath{destination})
		if err != nil {
			return fmt.Errorf("could not serve file: %w", err)
		}
		records, err = tbl.append(iter, records)
		if err != nil {
			return fmt.Errorf("could not append records to existing file: %w", err)
		}
	}
	data, err := tbl.writeRecordsToParquetBytes(records)
	if err != nil {
		return fmt.Errorf("could not write records to parquet bytes: %w", err)
	}
	return tbl.store.Write(destination, data)
}

// TODO: Add unit tests for this method
func (tbl *BlobOfflineTable) append(iter Iterator, newRecords []ResourceRecord) ([]ResourceRecord, error) {
	records := make([]ResourceRecord, 0)
	for {
		val, err := iter.Next()
		if err != nil {
			return nil, fmt.Errorf("could not iterate over file: %w", err)
		}
		if val == nil {
			break
		}
		record := ResourceRecord{
			Entity: val["Entity"].(string),
			Value:  val["Value"],
			TS:     val["TS"].(time.Time),
		}
		records = append(records, record)
	}
	records = append(records, newRecords...)
	return records, nil
}

// TODO: Add unit tests for this method
func (tbl *BlobOfflineTable) convertToGenericResourceRecord(record *ResourceRecord) (interface{}, error) {
	switch v := record.Value.(type) {
	case int:
		// **NOTE:** github.com/parquet-go/parquet-go does not support int, so this value was being cast to int64
		// at the time of writing to the filestore. This remains an issue for us given int is a valid ScalarType,
		// so we'll need to determine how to handle this.
		return &GenericResourceRecord[int32]{Entity: record.Entity, Value: int32(v), TS: record.TS}, nil
	case int32:
		return &GenericResourceRecord[int32]{Entity: record.Entity, Value: v, TS: record.TS}, nil
	case int64:
		return &GenericResourceRecord[int64]{Entity: record.Entity, Value: v, TS: record.TS}, nil
	case float32:
		return &GenericResourceRecord[float32]{Entity: record.Entity, Value: v, TS: record.TS}, nil
	case float64:
		return &GenericResourceRecord[float64]{Entity: record.Entity, Value: v, TS: record.TS}, nil
	case string:
		return &GenericResourceRecord[string]{Entity: record.Entity, Value: v, TS: record.TS}, nil
	case bool:
		return &GenericResourceRecord[bool]{Entity: record.Entity, Value: v, TS: record.TS}, nil
	case time.Time:
		return &GenericResourceRecord[time.Time]{Entity: record.Entity, Value: v, TS: record.TS}, nil
	default:
		return nil, fmt.Errorf("unsupported type: %T", v)
	}
}

// TODO: Add unit tests for this method
func (tbl *BlobOfflineTable) writeRecordsToParquetBytes(records []ResourceRecord) ([]byte, error) {
	parquetRecords := []any{}
	for _, record := range records {
		r, err := tbl.convertToGenericResourceRecord(&record)
		if err != nil {
			return nil, fmt.Errorf("could not convert record to generic resource record: %w", err)
		}
		parquetRecords = append(parquetRecords, r)
	}
	buf := new(bytes.Buffer)
	var schemaRecord interface{}
	// If there are no records, we still need to write the schema to the parquet file
	// to avoid Spark errors (e.g. "... failed to read footer ...") when reading the file.
	if len(parquetRecords) > 0 {
		schemaRecord = parquetRecords[0]
	} else {
		schemaRecord = &GenericResourceRecord[int16]{}
	}
	schema := parquet.SchemaOf(schemaRecord)
	err := parquet.Write[any](buf, parquetRecords, schema)
	if err != nil {
		return nil, fmt.Errorf("could not write parquet file to bytes: %v", err)
	}
	return buf.Bytes(), nil
}

func (k8s *K8sOfflineStore) RegisterResourceFromSourceTable(id ResourceID, schema ResourceSchema) (OfflineTable, error) {
	return blobRegisterResourceFromSourceTable(id, schema, k8s.logger, k8s.store)
}

func blobRegisterResourceFromSourceTable(id ResourceID, sourceSchema ResourceSchema, logger *zap.SugaredLogger, store FileStore) (OfflineTable, error) {
	if err := id.check(Feature, Label); err != nil {
		logger.Errorw("Failure checking ID", "error", err)
		return nil, fmt.Errorf("ID check failed: %v", err)
	}
	destination, err := store.CreateFilePath(id.ToFilestorePath())
	if err != nil {
		return nil, fmt.Errorf("could not create file path: %w", err)
	}
	resourceExists, err := store.Exists(destination)
	if err != nil {
		logger.Errorw("Error checking if resource exists", "error", err)
		return nil, fmt.Errorf("error checking if resource registry exists: %v", err)
	}
	if resourceExists {
		logger.Errorw("Resource already exists in blob store", "id", id, "ResourceKey", destination.Key())
		return nil, &TableAlreadyExists{id.Name, id.Variant}
	}
	serializedSchema, err := sourceSchema.Serialize()
	if err != nil {
		return nil, fmt.Errorf("error serializing resource schema: %s: %s", sourceSchema, err)
	}
	if err := store.Write(destination, serializedSchema); err != nil {
		return nil, fmt.Errorf("error writing resource schema: %s: %s", sourceSchema, err)
	}
	logger.Debugw("Registered resource table", "resourceID", id, "for source", sourceSchema.SourceTable)
	return &BlobOfflineTable{schema: sourceSchema, store: store}, nil
}

type FileStorePrimaryTable struct {
	store            FileStore
	source           filestore.Filepath
	schema           TableSchema
	isTransformation bool
	id               ResourceID
}

// TODO: implement
func (tbl *FileStorePrimaryTable) Write(record GenericRecord) error {
	return fmt.Errorf("not implemented")
}

func (tbl *FileStorePrimaryTable) WriteBatch(records []GenericRecord) error {
	destination, err := filestore.NewEmptyFilepath(tbl.store.FilestoreType())
	if err != nil {
		return fmt.Errorf("could not create file path: %w", err)
	}
	if err := destination.ParseFilePath(tbl.schema.SourceTable); err != nil {
		return fmt.Errorf("could not parse file path: %w", err)
	}
	exists, err := tbl.store.Exists(destination)
	if err != nil {
		return fmt.Errorf("could not check if destination file exists: %w", err)
	}
	if exists {
		iter, err := tbl.store.Serve([]filestore.Filepath{destination})
		if err != nil {
			return fmt.Errorf("could not serve file: %w", err)
		}
		records, err = tbl.append(iter, records)
		if err != nil {
			return fmt.Errorf("could not append records to existing file: %w", err)
		}
	}
	buf := new(bytes.Buffer)
	schema := parquet.SchemaOf(tbl.schema.Interface())
	parquetRecords := tbl.schema.ToParquetRecords(records)
	err = parquet.Write[any](buf, parquetRecords, schema)
	if err != nil {
		return fmt.Errorf("could not write parquet file to bytes: %v", err)
	}
	return tbl.store.Write(destination, buf.Bytes())
}

// TODO: Add unit tests for this method
func (tbl *FileStorePrimaryTable) append(iter Iterator, newRecords []GenericRecord) ([]GenericRecord, error) {
	records := make([]GenericRecord, 0)
	for {
		val, err := iter.Next()
		if err != nil {
			return nil, fmt.Errorf("could not iterate over file: %w", err)
		}
		if val == nil {
			break
		}
		record := GenericRecord{}
		for _, col := range tbl.schema.Columns {
			switch assertedVal := val[col.Name].(type) {
			case int32:
				record = append(record, int(assertedVal))
			case int64:
				// Given we're instructing Spark to output timestamps as int64 (microseconds),
				// we need to rely on the parquet schema's field metadata to determine whether
				// the field is a timestamp or not. If it is, we need to convert it to its
				// corresponding Go type (time.Time).
				if col.Scalar() == Timestamp {
					record = append(record, time.UnixMilli(assertedVal).UTC())
				} else {
					record = append(record, int(assertedVal))
				}
			default:
				record = append(record, assertedVal)
			}
		}
		records = append(records, record)
	}
	records = append(records, newRecords...)
	return records, nil
}

func (tbl *FileStorePrimaryTable) GetName() string {
	return tbl.source.ToURI()
}

func (tbl *FileStorePrimaryTable) IterateSegment(n int64) (GenericTableIterator, error) {
	sources := []filestore.Filepath{tbl.source}
	if tbl.source.IsDir() {
		// The key should only be a directory in the case of transformations.
		if !tbl.isTransformation {
			return nil, fmt.Errorf("expected a file but got a directory: %s", tbl.source.Key())
		}
		// The file structure in cloud storage for transformations is /featureform/Transformation/<NAME>/<VARIANT>
		// but there is an additional directory that's named using a timestamp that contains the transformation file
		// we need to access. NewestFileOfType will recursively search for the newest file of the given type (i.e.
		// parquet) given a path (i.e. `key`).
		transformations, err := tbl.store.List(tbl.source, filestore.Parquet)
		if err != nil {
			return nil, fmt.Errorf("could not find newest file of type %s: %w", filestore.Parquet, err)
		}
		groups, err := filestore.NewFilePathGroup(transformations, filestore.DateTimeDirectoryGrouping)
		if err != nil {
			return nil, fmt.Errorf("could not group files by datetime: %w", err)
		}
		newestFiles, err := groups.GetFirst()
		if err != nil {
			return nil, fmt.Errorf("could not get newest files: %w", err)
		}
		sources = newestFiles
	}
	fmt.Printf("Sources: %d found\n", len(sources))
	fmt.Printf("Source %s extension %s\n", sources[0].ToURI(), string(sources[0].Ext()))
	switch sources[0].Ext() {
	case filestore.Parquet:
		return newMultipleFileParquetIterator(sources, tbl.store, n)
	case filestore.CSV:
		if len(sources) > 1 {
			return nil, fmt.Errorf("multiple CSV files found for table (%v)", tbl.id)
		}
		fmt.Printf("Reading file at key %s in file store type %s\n", sources[0].Key(), tbl.store.FilestoreType())
		b, err := tbl.store.Read(sources[0])
		if err != nil {
			return nil, fmt.Errorf("could not read file: %w", err)
		}
		return newCSVIterator(b, n)
	default:
		return nil, fmt.Errorf("unsupported file type: %s", sources[0].Ext())
	}
}

func (tbl *FileStorePrimaryTable) NumRows() (int64, error) {
	src, err := tbl.GetSource()
	if err != nil {
		return 0, err
	}
	return tbl.store.NumRows(src)
}

func (tbl *FileStorePrimaryTable) GetSource() (filestore.Filepath, error) {
	filepath, err := filestore.NewEmptyFilepath(tbl.store.FilestoreType())
	if err != nil {
		return nil, err
	}
	err = filepath.ParseFilePath(tbl.schema.SourceTable)
	return filepath, err
}

func (k8s *K8sOfflineStore) RegisterPrimaryFromSourceTable(id ResourceID, sourcePath string) (PrimaryTable, error) {
	return blobRegisterPrimary(id, sourcePath, k8s.logger, k8s.store)
}

func blobRegisterPrimary(id ResourceID, sourcePath string, logger *zap.SugaredLogger, store FileStore) (PrimaryTable, error) {
	sourceFilePath, err := filestore.NewEmptyFilepath(store.FilestoreType())
	if err != nil {
		logger.Errorw("Could not create empty filepath", "error", err, "storeType", store.FilestoreType(), "sourcePath", sourcePath)
		return nil, err
	}
	err = sourceFilePath.ParseFilePath(sourcePath)
	if err != nil {
		logger.Errorw("Could not parse full path", "error", err, "sourcePath", sourcePath)
		return nil, err
	}
	storeExists, err := store.Exists(sourceFilePath)
	if err != nil {
		return nil, fmt.Errorf("error checking if source exists: %v", err)
	}
	if !storeExists {
		return nil, fmt.Errorf("source table does not exist")
	}

	filepath, err := store.CreateFilePath(id.ToFilestorePath())
	if err != nil {
		return nil, fmt.Errorf("could not create file path: %w", err)
	}
	logger.Infow("Checking if resource key exists", "key", filepath.Key())
	primaryExists, err := store.Exists(filepath)
	if err != nil {
		logger.Errorw("Error checking if primary exists", "error", err)
		return nil, fmt.Errorf("failed to check if source exists: %v", err)
	}
	if primaryExists {
		logger.Errorw("File already registered", "source", sourcePath)
		return nil, fmt.Errorf("source has already been registered: %s", sourcePath)
	}
	logger.Debugw("Registering primary table", "id", id, "source", sourcePath)
	// TODO: determine how to handle the schema of the primary table; if it's a parquet file, we _could_
	// read the file and infer a schema based; however, this wouldn't be possible for CSV files.
	schema := TableSchema{
		SourceTable: sourcePath,
	}
	data, err := schema.Serialize()
	if err != nil {
		return nil, fmt.Errorf("error serializing primary schema: %s: %s", schema, err)
	}
	// **NOTE:** The data we're writing to the blob store is the path to the primary source data file.
	// This blob will be read by other processes (e.g. transformation jobs) to fetch where the primary
	// data is stored prior to acting on it. You can verify this by accessing the object stored at
	// /featureform/Primary/<NAME>/<VARIANT>
	if err := store.Write(filepath, data); err != nil {
		logger.Errorw("Could not write primary table", "error", err)
		return nil, err
	}

	logger.Debugw("Successfully registered primary table", "id", id, "source", sourcePath)
	return &FileStorePrimaryTable{store, sourceFilePath, schema, false, id}, nil
}

func (k8s *K8sOfflineStore) CreateTransformation(config TransformationConfig) error {
	return k8s.transformation(config, false)
}

func (k8s *K8sOfflineStore) transformation(config TransformationConfig, isUpdate bool) error {
	if config.Type == SQLTransformation {
		return k8s.sqlTransformation(config, isUpdate)
	} else if config.Type == DFTransformation {
		return k8s.dfTransformation(config, isUpdate)
	} else {
		k8s.logger.Errorw("the transformation type is not supported", "type", config.Type)
		return fmt.Errorf("the transformation type '%v' is not supported", config.Type)
	}
}

func addETCDVars(envVars map[string]string) map[string]string {
	etcdHost := helpers.GetEnv("ETCD_HOST", "localhost")
	etcdPort := helpers.GetEnv("ETCD_PORT", "2379")
	etcdPassword := helpers.GetEnv("ETCD_PASSWORD", "secretpassword")
	etcdUsername := helpers.GetEnv("ETCD_USERNAME", "root")
	envVars["ETCD_HOST"] = etcdHost
	envVars["ETCD_PASSWORD"] = etcdPassword
	envVars["ETCD_PORT"] = etcdPort
	envVars["ETCD_USERNAME"] = etcdUsername
	return envVars
}

func (k8s *K8sOfflineStore) pandasRunnerArgs(outputURI string, updatedQuery string, sources []string, jobType JobType) map[string]string {
	sourceList := strings.Join(sources, ",")
	envVars := map[string]string{
		"OUTPUT_URI":          outputURI,
		"SOURCES":             sourceList,
		"TRANSFORMATION_TYPE": "sql",
		"TRANSFORMATION":      updatedQuery,
	}
	envVars = k8s.store.AddEnvVars(envVars)
	return envVars
}

func (k8s K8sOfflineStore) getDFArgs(outputURI string, code string, mapping []SourceMapping, sources []string) map[string]string {
	sourceList := strings.Join(sources, ",")
	envVars := map[string]string{
		"OUTPUT_URI":          outputURI,
		"SOURCES":             sourceList,
		"TRANSFORMATION_TYPE": "df",
		"TRANSFORMATION":      code,
	}
	envVars = k8s.store.AddEnvVars(envVars)
	return envVars
}

func addResourceID(envVars map[string]string, id ResourceID) map[string]string {
	envVars["RESOURCE_NAME"] = id.Name
	envVars["RESOURCE_VARIANT"] = id.Variant
	envVars["RESOURCE_TYPE"] = fmt.Sprintf("%d", id.Type)
	return envVars
}

func (k8s *K8sOfflineStore) sqlTransformation(config TransformationConfig, isUpdate bool) error {
	updatedQuery, sources, err := k8s.updateQuery(config.Query, config.SourceMapping)
	if err != nil {
		k8s.logger.Errorw("Could not generate updated query for k8s transformation", err)
		return err
	}

	filepath, err := k8s.store.CreateFilePath(fileStoreResourcePath(config.TargetTableID))
	if err != nil {
		return fmt.Errorf("could not create file path: %w", err)
	}
	transformationDestinationExactPath, err := k8s.store.NewestFileOfType(filepath, filestore.Parquet)
	if err != nil {
		k8s.logger.Errorw("Could not get newest blob", "location", filepath.Key(), "error", err)
		return fmt.Errorf("could not get newest blob: %s: %v", filepath.Key(), err)
	}
	exists := transformationDestinationExactPath != nil
	if !isUpdate && exists {
		k8s.logger.Errorw("Creation when transformation already exists", "target_table", config.TargetTableID, "destination", filepath.ToURI())
		return fmt.Errorf("transformation %v already exists at %s", config.TargetTableID, filepath)
	} else if isUpdate && !exists {
		k8s.logger.Errorw("Update job attempted when transformation does not exist", "target_table", config.TargetTableID, "destination", filepath.ToURI())
		return fmt.Errorf("transformation %v doesn't exist at %s and you are trying to update", config.TargetTableID, filepath.ToURI())
	}
	k8s.logger.Debugw("Running SQL transformation", "target_table", config.TargetTableID, "query", config.Query)
	runnerArgs := k8s.pandasRunnerArgs(filepath.ToURI(), updatedQuery, sources, Transform)
	runnerArgs = addResourceID(runnerArgs, config.TargetTableID)

	args, err := k8s.checkArgs(config.Args)
	if err != nil {
		return fmt.Errorf("could not check args: %w", err)
	}
	if err := k8s.executor.ExecuteScript(runnerArgs, &args); err != nil {
		k8s.logger.Errorw("job for transformation failed to run", "target_table", config.TargetTableID, "error", err)
		return fmt.Errorf("job for transformation %v failed to run: %v", config.TargetTableID, err)
	}

	k8s.logger.Debugw("Successfully ran SQL transformation", "target_table", config.TargetTableID, "query", config.Query)
	return nil
}

func (k8s *K8sOfflineStore) checkArgs(args metadata.TransformationArgs) (metadata.KubernetesArgs, error) {
	k8sArgs, ok := args.(metadata.KubernetesArgs)
	if !ok {
		return metadata.KubernetesArgs{}, fmt.Errorf("invalid type used for Kubernetes Arguments")
	}
	return k8sArgs, nil
}

func (k8s *K8sOfflineStore) dfTransformation(config TransformationConfig, isUpdate bool) error {
	_, sources, err := k8s.updateQuery(config.Query, config.SourceMapping)
	if err != nil {
		return err
	}
	filepath, err := k8s.store.CreateFilePath(fileStoreResourcePath(config.TargetTableID))
	if err != nil {
		return fmt.Errorf("could not create file path: %w", err)
	}
	exists, err := k8s.store.Exists(filepath)
	if err != nil {
		k8s.logger.Errorw("Error checking if resource exists", "error", err)
		return err
	}

	if !isUpdate && exists {
		k8s.logger.Errorw("Transformation already exists", "target_table", config.TargetTableID, "destination", filepath.ToURI())
		return fmt.Errorf("transformation %v already exists at %s", config.TargetTableID, filepath.ToURI())
	} else if isUpdate && !exists {
		k8s.logger.Errorw("Transformation doesn't exists at destination and you are trying to update", "target_table", config.TargetTableID, "destination", filepath.ToURI())
		return fmt.Errorf("transformation %v doesn't exist at %s and you are trying to update", config.TargetTableID, filepath.ToURI())
	}

	key := fmt.Sprintf("%s%s", fileStoreResourcePath(config.TargetTableID), "transformation.pkl")
	transformationFilepath, err := k8s.store.CreateFilePath(key)
	if err != nil {
		return fmt.Errorf("could not create file path to transformation file: %w", err)
	}
	err = k8s.store.Write(transformationFilepath, config.Code)
	if err != nil {
		return fmt.Errorf("could not upload file: %v", err)
	}

	dfArgs := k8s.getDFArgs(filepath.ToURI(), transformationFilepath.Key(), config.SourceMapping, sources)
	dfArgs = addResourceID(dfArgs, config.TargetTableID)
	k8s.logger.Debugw("Running DF transformation", "target_table", config.TargetTableID)
	args, err := k8s.checkArgs(config.Args)
	if err != nil {
		return fmt.Errorf("could not check args: %w", err)
	}
	if err := k8s.executor.ExecuteScript(dfArgs, &args); err != nil {
		k8s.logger.Errorw("Error running dataframe job", "error", err)
		return fmt.Errorf("submit job for transformation %v failed to run: %v", config.TargetTableID, err)
	}

	k8s.logger.Debugw("Successfully ran DF transformation", "target_table", config.TargetTableID)
	return nil
}

func (k8s *K8sOfflineStore) updateQuery(query string, mapping []SourceMapping) (string, []string, error) {
	sources := make([]string, len(mapping))
	replacements := make([]string, len(mapping)*2) // It's times 2 because each replacement will be a pair; (original, replacedValue)

	for i, m := range mapping {
		replacements = append(replacements, m.Template)
		replacements = append(replacements, fmt.Sprintf("source_%v", i))

		sourcePath := ""
		sourcePath, err := k8s.getSourcePath(m.Source)
		k8s.logger.Debugw("Fetched Source Path", "source", m.Source, "sourcePath", sourcePath)
		if err != nil {
			k8s.logger.Errorw("Error getting source path of source", "source", m.Source, "error", err)
			return "", nil, fmt.Errorf("could not get the sourcePath for %s because %s", m.Source, err)
		}

		sources[i] = sourcePath
	}

	replacer := strings.NewReplacer(replacements...)
	updatedQuery := replacer.Replace(query)

	if strings.Contains(updatedQuery, "{{") {
		k8s.logger.Errorw("Template replace failed", "query", updatedQuery)
		return "", nil, fmt.Errorf("could not replace all the templates with the current mapping. Mapping: %v; Replaced Query: %s", mapping, updatedQuery)
	}
	return updatedQuery, sources, nil
}

// TODO: Determine if this should return filestore.Filepath or string
func (k8s *K8sOfflineStore) getSourcePath(path string) (string, error) {
	fileType, fileName, fileVariant := k8s.getResourceInformationFromFilePath(path)
	k8s.logger.Debugw("Retrieved source path", "fileType", fileType, "fileName", fileName, "fileVariant", fileVariant)

	var filePath string
	if fileType == "primary" {
		fileResourceId := ResourceID{Name: fileName, Variant: fileVariant, Type: Primary}
		fileTable, err := k8s.GetPrimaryTable(fileResourceId)
		if err != nil {
			k8s.logger.Errorw("Issue getting primary table", "id", fileResourceId, "error", err)
			return "", fmt.Errorf("could not get the primary table for {%v} because %s", fileResourceId, err)
		}
		filePath = fileTable.GetName()
		return filePath, nil
	} else if fileType == "transformation" {
		fileResourceId := ResourceID{Name: fileName, Variant: fileVariant, Type: Transformation}
		fileResourcePath, err := k8s.store.CreateFilePath(fileStoreResourcePath(fileResourceId))
		if err != nil {
			k8s.logger.Errorw("Could not create file path", "error", err, "fileStoreResourcePath", fileStoreResourcePath(fileResourceId))
			return "", err
		}
		k8s.logger.Debugw("Retrieved transformation source", "ResourceId", fileResourceId, "fileResourcePath", fileResourcePath)
		// get file type of source
		exactFileResourcePath, err := k8s.store.NewestFileOfType(fileResourcePath, fileResourcePath.Ext())
		k8s.logger.Debugw("Retrieved latest file path", "exactFileResourcePath", exactFileResourcePath)
		if err != nil {
			k8s.logger.Errorw("Could not get newest blob", "location", fileResourcePath, "error", err)
			return "", fmt.Errorf("could not get newest blob: %s: %v", fileResourcePath, err)
		}
		exists, err := k8s.store.Exists(exactFileResourcePath)
		if err != nil {
			k8s.logger.Errorw("Issue getting transformation table", "id", fileResourceId)
			return "", fmt.Errorf("could not get the transformation table for {%v} at {%s}", fileResourceId, fileResourcePath)
		}
		if !exists {
			k8s.logger.Errorw("Transformation table does not exist", "id", fileResourceId)
			return "", fmt.Errorf("expected transformation {%v} at {%s} does not exist", fileResourceId, fileResourcePath)
		}
		return exactFileResourcePath.ToURI(), nil
	} else {
		return filePath, fmt.Errorf("could not find path for %s; fileType: %s, fileName: %s, fileVariant: %s", path, fileType, fileName, fileVariant)
	}
}

func (k8s *K8sOfflineStore) getResourceInformationFromFilePath(path string) (string, string, string) {
	var fileType string
	var fileName string
	var fileVariant string
	if path[:5] == "s3://" {
		filePaths := strings.Split(path[len("s3://"):], "/")
		if len(filePaths) <= 4 {
			return "", "", ""
		}
		fileType, fileName, fileVariant = strings.ToLower(filePaths[2]), filePaths[3], filePaths[4]
	} else if path[:5] == filestore.HDFSPrefix {
		filePaths := strings.Split(path[len(filestore.HDFSPrefix):], "/")
		if len(filePaths) <= 4 {
			return "", "", ""
		}
		fileType, fileName, fileVariant = strings.ToLower(filePaths[2]), filePaths[3], filePaths[4]
	} else {
		filePaths := strings.Split(path[len("featureform_"):], "__")
		if len(filePaths) <= 2 {
			return "", "", ""
		}
		fileType, fileName, fileVariant = filePaths[0], filePaths[1], filePaths[2]
	}
	return fileType, fileName, fileVariant
}

func (k8s *K8sOfflineStore) GetTransformationTable(id ResourceID) (TransformationTable, error) {
	transformationFilepath, err := k8s.store.CreateFilePath(fileStoreResourcePath(id))
	if err != nil {
		k8s.logger.Errorw("Could not create file path", "error", err, "fileStoreResourcePath", fileStoreResourcePath(id))
		return nil, err
	}
	k8s.logger.Debugw("Retrieved transformation source", "ResourceId", id, "transformationPath", transformationFilepath.ToURI())
	if err != nil {
		k8s.logger.Errorw("Could not parse full path", "error", err, "transformationPath", transformationFilepath.ToURI())
		return nil, err
	}
	if err != nil {
		k8s.logger.Errorw("Could not create empty filepath", "error", err, "storeType", k8s.store.FilestoreType(), "transformationPath", transformationFilepath.ToURI())
		return nil, err
	}
	// TODO: populate schema
	return &FileStorePrimaryTable{k8s.store, transformationFilepath, TableSchema{}, true, id}, nil
}

func (k8s *K8sOfflineStore) UpdateTransformation(config TransformationConfig) error {
	return k8s.transformation(config, true)
}
func (k8s *K8sOfflineStore) CreatePrimaryTable(id ResourceID, schema TableSchema) (PrimaryTable, error) {
	return nil, fmt.Errorf("not implemented")
}

func (k8s *K8sOfflineStore) GetPrimaryTable(id ResourceID) (PrimaryTable, error) {
	return fileStoreGetPrimary(id, k8s.store, k8s.logger)
}

func fileStoreGetPrimary(id ResourceID, store FileStore, logger *zap.SugaredLogger) (PrimaryTable, error) {
	filepath, err := store.CreateFilePath(id.ToFilestorePath())
	if err != nil {
		return nil, fmt.Errorf("could not create file path: %w", err)
	}
	logger.Debugw("Getting primary table", "id", id, "resourceKey", filepath.Key())
	table, err := store.Read(filepath)
	logger.Debugw("Read primary table", "table", string(table), "error", err)
	if err != nil {
		return nil, fmt.Errorf("error fetching primary table: %v", err)
	}
	logger.Debugw("Successfully retrieved primary table", "id", id)
	schema := TableSchema{}
	if err := schema.Deserialize(table); err != nil {
		return nil, fmt.Errorf("error deserializing primary table: %v", err)
	}
	sourcePath, err := filestore.NewEmptyFilepath(store.FilestoreType())
	if err != nil {
		return nil, fmt.Errorf("could not create empty filepath for source: %w", err)
	}
	if err := sourcePath.ParseFilePath(schema.SourceTable); err != nil {
		return nil, fmt.Errorf("could not parse source table: %w", err)
	}
	return &FileStorePrimaryTable{store, sourcePath, schema, false, id}, nil
}

func (k8s *K8sOfflineStore) CreateResourceTable(id ResourceID, schema TableSchema) (OfflineTable, error) {
	return nil, fmt.Errorf("not implemented")
}

func (k8s *K8sOfflineStore) GetResourceTable(id ResourceID) (OfflineTable, error) {
	return fileStoreGetResourceTable(id, k8s.store, k8s.logger)
}

func fileStoreGetResourceTable(id ResourceID, store FileStore, logger *zap.SugaredLogger) (OfflineTable, error) {
	filepath, err := store.CreateFilePath(id.ToFilestorePath())
	if err != nil {
		return nil, fmt.Errorf("could not create file path: %w", err)
	}
	if exists, err := store.Exists(filepath); err != nil {
		return nil, fmt.Errorf("could not check if table exists: %v", err)
	} else if !exists {
		return nil, &TableNotFound{id.Name, id.Variant}
	}
	logger.Debugw("Getting resource table", "id", id, "resourceKey", filepath.Key())
	serializedSchema, err := store.Read(filepath)
	if err != nil {
		return nil, fmt.Errorf("error reading schema bytes from blob storage: %v", err)
	}
	resourceSchema := ResourceSchema{}
	if err := resourceSchema.Deserialize(serializedSchema); err != nil {
		return nil, fmt.Errorf("error deserializing resource table: %v", err)
	}
	if store.FilestoreType() == filestore.GCS {
		src, err := completePrimarySourceTablePathForGCS(resourceSchema.SourceTable, store)
		if err != nil {
			return nil, fmt.Errorf("could not complete primary source table path for GCS: %w", err)
		}
		if src != nil {
			resourceSchema.SourceTable = src.ToURI()
		}
	}
	logger.Debugw("Successfully fetched resource table", "id", id)
	return &BlobOfflineTable{schema: resourceSchema, store: store}, nil
}

// **NOTE:** This is a temporary fix for GCS. The primary source table path is stored in the resource schema,
// which will contain only a directory path for resources other than a primary source. For example, or a Label that was
// created from a primary source, the value for `SourceTable` on the label schema will be:
// `featureform/Primary/<NAME>/<VARIANT>`
// This has been the pattern for all file stores given that Spark can recursively discover files "under" this
// directory path; however, in the case of GCS, Spark was interpreting this source path _as_ a parquet file,
// which results in a failure to read the file. To fix this, we need to get the remainder of the primary source's
// path to the parquet file and append it to the source path stored in the resource schema.
func completePrimarySourceTablePathForGCS(sourceTable string, store FileStore) (filestore.Filepath, error) {
	srcID := ResourceID{}
	if err := srcID.FromFilestorePath(sourceTable); err != nil {
		return nil, fmt.Errorf("could not parse resource table to id: %v", err)
	}
	if srcID.Type == Primary {
		srcPath, err := store.CreateDirPath(srcID.ToFilestorePath())
		if err != nil {
			return nil, fmt.Errorf("could not create dir path: %w", err)
		}
		src, err := store.NewestFileOfType(srcPath, filestore.Parquet)
		if err != nil {
			return nil, fmt.Errorf("could not get newest blob from GCS: %s: %v", srcPath, err)
		}
		return src, nil
	}
	return nil, nil
}

func (k8s *K8sOfflineStore) CreateMaterialization(id ResourceID) (Materialization, error) {
	return k8s.materialization(id, false)
}

func (k8s *K8sOfflineStore) GetMaterialization(id MaterializationID) (Materialization, error) {
	return fileStoreGetMaterialization(id, k8s.store, k8s.logger)
}

func fileStoreGetMaterialization(id MaterializationID, store FileStore, logger *zap.SugaredLogger) (Materialization, error) {
	s := strings.Split(string(id), "/")
	if len(s) != 3 {
		logger.Errorw("Invalid materialization", "id", id)
		return nil, fmt.Errorf("invalid materialization id: %v", id)
	}
	materializationID := ResourceID{s[1], s[2], FeatureMaterialization}
	logger.Debugw("Getting materialization", "id", id)
	logger.Debugw("Successfully retrieved materialization", "id", id)
	return &FileStoreMaterialization{materializationID, store}, nil
}

type FileStoreMaterialization struct {
	id    ResourceID
	store FileStore
}

func (mat FileStoreMaterialization) ID() MaterializationID {
	return MaterializationID(fmt.Sprintf("%s/%s/%s", FeatureMaterialization, mat.id.Name, mat.id.Variant))
}

func (mat FileStoreMaterialization) NumRows() (int64, error) {
	materializationFilepath, err := mat.store.CreateFilePath(fileStoreResourcePath(mat.id))
	if err != nil {
		return 0, fmt.Errorf("could not create file path: %w", err)
	}
	latestMaterializationPath, err := mat.store.NewestFileOfType(materializationFilepath, filestore.Parquet)
	if err != nil {
		return 0, fmt.Errorf("could not get materialization num rows; %v", err)
	}
	// TODO: convert NumRows to accept a list of files to get a total count
	return mat.store.NumRows(latestMaterializationPath)
}

func (mat FileStoreMaterialization) IterateSegment(begin, end int64) (FeatureIterator, error) {
	searchPath, err := mat.store.CreateFilePath(fileStoreResourcePath(mat.id))
	if err != nil {
		return nil, fmt.Errorf("could not create file path: %w", err)
	}
	files, err := mat.store.List(searchPath, filestore.Parquet)
	if err != nil {
		return nil, fmt.Errorf("could not get materialization iterate segment: %v", err)
	}
	groups, err := filestore.NewFilePathGroup(files, filestore.DateTimeDirectoryGrouping)
	if err != nil {
		return nil, fmt.Errorf("could not groups files by datetime directory: %v", err)
	}
	newestFiles, err := groups.GetFirst()
	if err != nil {
		return nil, fmt.Errorf("could not get newest files: %v", err)
	}
	iter, err := mat.store.Serve(newestFiles)
	if err != nil {
		return nil, err
	}
	i := int64(0)
	for i = 0; i < begin; i++ {
		_, _ = iter.Next()
	}
	return &FileStoreFeatureIterator{
		iter:   iter,
		curIdx: i,
		maxIdx: end,
	}, nil
}

type FileStoreFeatureIterator struct {
	iter   Iterator
	err    error
	cur    ResourceRecord
	curIdx int64
	maxIdx int64
}

func (iter *FileStoreFeatureIterator) Next() bool {
	iter.curIdx += 1
	if iter.curIdx > iter.maxIdx {
		return false
	}
	nextVal, err := iter.iter.Next()
	if err != nil {
		iter.err = err
		return false
	}
	if nextVal == nil {
		return false
	}
	value, err := iter.parseValue(nextVal["value"])
	if err != nil {
		iter.err = err
		return false
	}
	timestamp := time.UnixMilli(0).UTC()
	ts, hasTimestamp := nextVal["ts"]
	if hasTimestamp {
		if castedTS, err := castToTimestamp(ts); err != nil {
			iter.err = err
			return false
		} else {
			timestamp = castedTS
		}
	}
	rec := ResourceRecord{
		Value: value,
		TS:    timestamp,
	}
	if err := rec.SetEntity(nextVal["entity"]); err != nil {
		iter.err = err
		return false
	}
	iter.cur = rec
	return true
}

func castToTimestamp(timestamp interface{}) (time.Time, error) {
	if ts, ok := timestamp.(time.Time); !ok {
		return time.UnixMilli(0).UTC(), fmt.Errorf("expected timestamp to be of type time.Time, but got %T", timestamp)
	} else {
		return ts, nil
	}
}

// Attempts to parse value in one of the following formats:
// 1. a scalar value (string, int, float, bool)
// 2. []float32 (i.e. vector32)
func (iter *FileStoreFeatureIterator) parseValue(value interface{}) (interface{}, error) {
	valueMap, ok := value.(map[string]interface{})
	if !ok {
		if value, ok := value.(int32); ok {
			return int(value), nil
		}
		return value, nil
	}
	list, ok := valueMap["list"]
	if !ok {
		return "", fmt.Errorf("expected to find field 'list' value (type %T)", value)
	}
	// To iterate over the list and create a we need to cast it to []interface{}
	elementsSlice, ok := list.([]interface{})
	if !ok {
		return "", fmt.Errorf("could not cast type: %T to []interface{}", list)
	}
	vector32 := make([]float32, len(elementsSlice))
	for i, e := range elementsSlice {
		// To access the 'element' field, which holds the float value,
		// we need to cast it to map[string]interface{}
		m, ok := e.(map[string]interface{})
		if !ok {
			return "", fmt.Errorf("could not cast type: %T to map[string]interface{}", e)
		}
		switch element := m["element"].(type) {
		case float32:
			vector32[i] = element
		// Given floats in Python are typically 64-bit, it's possible we'll receive
		// a vector of float64
		case float64:
			vector32[i] = float32(element)
		default:
			return "", fmt.Errorf("unexpected type in parquet vector list: %T", element)
		}
	}
	return vector32, nil
}

func (iter *FileStoreFeatureIterator) Value() ResourceRecord {
	return iter.cur
}

func (iter *FileStoreFeatureIterator) Err() error {
	return iter.err
}

func (iter *FileStoreFeatureIterator) Close() error {
	return nil
}

func (k8s *K8sOfflineStore) UpdateMaterialization(id ResourceID) (Materialization, error) {
	return k8s.materialization(id, true)
}

func (k8s *K8sOfflineStore) materialization(id ResourceID, isUpdate bool) (Materialization, error) {
	if id.Type != Feature {
		k8s.logger.Errorw("Attempted to create a materialization of a non feature resource", "type", id.Type)
		return nil, fmt.Errorf("only features can be materialized")
	}
	resourceTable, err := k8s.GetResourceTable(id)
	if err != nil {
		k8s.logger.Errorw("Attempted to fetch resource table of non registered resource", "error", err)
		return nil, fmt.Errorf("resource not registered: %v", err)
	}
	k8sResourceTable, ok := resourceTable.(*BlobOfflineTable)
	if !ok {
		k8s.logger.Errorw("Could not convert resource table to blob offline table", "id", id)
		return nil, fmt.Errorf("could not convert offline table with id %v to k8sResourceTable", id)
	}
	materializationID := ResourceID{Name: id.Name, Variant: id.Variant, Type: FeatureMaterialization}
	featureResourcePath := fileStoreResourcePath(materializationID)
	destinationPath, err := k8s.store.CreateFilePath(featureResourcePath)
	if err != nil {
		k8s.logger.Errorw("Could not create file path", "error", err, "featureResourcePath", featureResourcePath)
		return nil, err
	}
	materializationNewestFile, err := k8s.store.NewestFileOfType(destinationPath, filestore.Parquet)
	k8s.logger.Debugw("Running Materialization", "id", id, "destinationPath", destinationPath, "materializationNewestFile", materializationNewestFile)
	materializationExists, err := k8s.store.Exists(materializationNewestFile)
	if err != nil {
		k8s.logger.Errorw("Could not determine whether materialization exists", err)
		return nil, fmt.Errorf("error checking if materialization exists: %v", err)
	}
	if !isUpdate && materializationExists {
		k8s.logger.Errorw("Attempted to materialize a materialization that already exists", "id", id)
		return nil, fmt.Errorf("materialization already exists")
	} else if isUpdate && !materializationExists {
		k8s.logger.Errorw("Attempted to update a materialization that does not exist", "id", id)
		return nil, fmt.Errorf("materialization does not exist")
	}
	materializationQuery := k8s.query.materializationCreate(k8sResourceTable.schema)
	sourcePath, err := k8s.store.CreateFilePath(k8sResourceTable.schema.SourceTable)
	if err != nil {
		k8s.logger.Errorw("Could not create file path", "error", err, "sourceTable", k8sResourceTable.schema.SourceTable)
		return nil, err
	}
	// get source path file type; note, it's possible it doesn't have it
	newestSourcePath, err := k8s.store.NewestFileOfType(sourcePath, sourcePath.Ext())
	k8s.logger.Debugw("Retrieved newest source path", "sourcePath", sourcePath, "newestSourcePath", newestSourcePath)
	if err != nil {
		k8s.logger.Errorw("Could not determine newest source file for materialization", "sourcePath", sourcePath, "error", err)
		return nil, fmt.Errorf("error determining newest source file: %v", err)
	}
	k8sArgs := k8s.pandasRunnerArgs(destinationPath.ToURI(), materializationQuery, []string{newestSourcePath.ToURI()}, Materialize)
	k8sArgs = addResourceID(k8sArgs, id)
	if err := k8s.executor.ExecuteScript(k8sArgs, nil); err != nil {
		k8s.logger.Errorw("Job failed to run", "error", err)
		return nil, fmt.Errorf("job for materialization %v failed to run: %v", materializationID, err)
	}

	k8s.logger.Debugw("Successfully created materialization", "id", id)
	return &FileStoreMaterialization{materializationID, k8s.store}, nil
}

func (k8s *K8sOfflineStore) DeleteMaterialization(id MaterializationID) error {
	return fileStoreDeleteMaterialization(id, k8s.store, k8s.logger)
}

func fileStoreDeleteMaterialization(id MaterializationID, store FileStore, logger *zap.SugaredLogger) error {
	s := strings.Split(string(id), "/")
	if len(s) != 3 {
		logger.Errorw("Invalid materialization id", id)
		return &MaterializationNotFound{id}
	}
	materializationID := ResourceID{s[1], s[2], FeatureMaterialization}
	materializationPath, err := store.CreateFilePath(fileStoreResourcePath(materializationID))
	if err != nil {
		return fmt.Errorf("could not create file path for materialization %v: %w", id, err)
	}
	exits, err := store.Exists(materializationPath)
	if err != nil {
		return fmt.Errorf("could not check if materialization %v exists: %v", materializationID, err)
	}
	if !exits {
		return &MaterializationNotFound{id}
	}
	return store.DeleteAll(materializationPath)
}

func (k8s *K8sOfflineStore) CreateTrainingSet(def TrainingSetDef) error {
	return k8s.trainingSet(def, false)
}

func (k8s *K8sOfflineStore) UpdateTrainingSet(def TrainingSetDef) error {
	return k8s.trainingSet(def, true)
}

func (k8s *K8sOfflineStore) registeredResourceSchema(id ResourceID) (ResourceSchema, error) {
	k8s.logger.Debugw("Getting resource schema", "id", id)
	table, err := k8s.GetResourceTable(id)
	if err != nil {
		k8s.logger.Errorw("Resource not registered in blob store", "id", id, "error", err)
		return ResourceSchema{}, fmt.Errorf("resource not registered: %v", err)
	}
	blobResourceTable, ok := table.(*BlobOfflineTable)
	if !ok {
		k8s.logger.Errorw("could not convert offline table to blobResourceTable", "id", id)
		return ResourceSchema{}, fmt.Errorf("could not convert offline table with id %v to blobResourceTable", id)
	}
	k8s.logger.Debugw("Successfully retrieved resource schema", "id", id, "schema", blobResourceTable.schema)
	return blobResourceTable.schema, nil
}

func (k8s *K8sOfflineStore) trainingSet(def TrainingSetDef, isUpdate bool) error {
	if err := def.check(); err != nil {
		k8s.logger.Errorw("Training set definition not valid", def, err)
		return err
	}
	sourcePaths := make([]string, 0)
	featureSchemas := make([]ResourceSchema, 0)
	destinationPath, err := k8s.store.CreateFilePath(fileStoreResourcePath(def.ID))
	if err != nil {
		return fmt.Errorf("could not create file path: %w", err)
	}
	trainingSetExactPath, err := k8s.store.NewestFileOfType(destinationPath, filestore.Parquet)

	k8s.logger.Debugw("Running Training Set", "id", def.ID, "destinationPath", destinationPath, "trainingSetExactPath", trainingSetExactPath)

	if err != nil {
		return fmt.Errorf("could not get training set path: %v", err)
	}
	trainingSetExists, err := k8s.store.Exists(trainingSetExactPath)
	if err != nil {
		k8s.logger.Errorw("Could not determine whether training set exists", err)
		return fmt.Errorf("error checking if training set exists: %v", err)
	}
	if !isUpdate && trainingSetExists {
		k8s.logger.Errorw("Training set already exists", "id", def.ID)
		return fmt.Errorf("k8s training set already exists: %v", def.ID)
	} else if isUpdate && !trainingSetExists {
		k8s.logger.Errorw("Training set doesn't exist for update job", def.ID)
		return fmt.Errorf("training set doesn't exist for update job: %v", def.ID)
	}
	labelSchema, err := k8s.registeredResourceSchema(def.Label)
	if err != nil {
		k8s.logger.Errorw("Could not get schema of label in store", "id", def.Label, "error", err)
		return fmt.Errorf("could not get schema of label %s: %v", def.Label, err)
	}
	labelPath := labelSchema.SourceTable
	labelFilepath, err := k8s.store.CreateFilePath(labelPath)
	if err != nil {
		k8s.logger.Errorw("Could not create file path", "error", err, "labelPath", labelPath)
		return fmt.Errorf("could not create file path: %v", err)
	}
	latestLabelFile, err := k8s.store.NewestFileOfType(labelFilepath, labelFilepath.Ext())
	k8s.logger.Debugw("Latest label file", "labelPath", labelPath, "latestLabelFile", latestLabelFile)
	if err != nil {
		k8s.logger.Errorw("Could not get latest label file", "error", err)
		return fmt.Errorf("could not get latest label file: %v", err)
	}
	sourcePaths = append(sourcePaths, labelFilepath.ToURI())
	for _, feature := range def.Features {
		featureSchema, err := k8s.registeredResourceSchema(feature)
		if err != nil {
			k8s.logger.Errorw("Could not get schema of feature in store", "feature", feature, "error", err)
			return fmt.Errorf("could not get schema of feature %s: %v", feature, err)
		}
		featurePath := featureSchema.SourceTable
		featureFilepath, err := k8s.store.CreateFilePath(featurePath)
		if err != nil {
			k8s.logger.Errorw("Could not create file path", "error", err, "featurePath", featurePath)
			return fmt.Errorf("could not create file path: %v", err)
		}
		latestFeatureFile, err := k8s.store.NewestFileOfType(featureFilepath, featureFilepath.Ext())
		k8s.logger.Debugw("Latest feature file", "featurePath", featurePath, "latestFeatureFile", latestFeatureFile)
		if err != nil {
			k8s.logger.Errorw("Could not get latest feature file", "error", err)
			return fmt.Errorf("could not get latest feature file: %v", err)
		}
		sourcePaths = append(sourcePaths, featureFilepath.ToURI())
		featureSchemas = append(featureSchemas, featureSchema)
	}
	trainingSetQuery := k8s.query.trainingSetCreate(def, featureSchemas, labelSchema)
	k8s.logger.Debugw("Source List", "SourceFiles", sourcePaths)
	k8s.logger.Debugw("Training Set Query", "list", trainingSetQuery)
	pandasArgs := k8s.pandasRunnerArgs(destinationPath.ToURI(), trainingSetQuery, sourcePaths, CreateTrainingSet)
	pandasArgs = addResourceID(pandasArgs, def.ID)
	k8s.logger.Debugw("Creating training set", "definition", def)

	if err := k8s.executor.ExecuteScript(pandasArgs, nil); err != nil {
		k8s.logger.Errorw("training set job failed to run", "definition", def.ID, "error", err)
		return fmt.Errorf("job for training set %v failed to run: %v", def.ID, err)
	}
	k8s.logger.Debugw("Successfully created training set:", "definition", def)
	return nil
}

func (k8s *K8sOfflineStore) GetTrainingSet(id ResourceID) (TrainingSetIterator, error) {
	return fileStoreGetTrainingSet(id, k8s.store, k8s.logger)
}

func (k8s *K8sOfflineStore) CheckHealth() (bool, error) {
	return false, fmt.Errorf("provider health check not implemented")
}

func fileStoreGetTrainingSet(id ResourceID, store FileStore, logger *zap.SugaredLogger) (TrainingSetIterator, error) {
	if err := id.check(TrainingSet); err != nil {
		logger.Errorw("Resource is not of type training set", "error", err)
		return nil, fmt.Errorf("resource is not training set: %w", err)
	}
	filepath, err := store.CreateFilePath(fileStoreResourcePath(id))
	if err != nil {
		logger.Errorw("Could not create file path", "error", err)
		return nil, fmt.Errorf("could not create file path: %w", err)
	}
	trainingSetExists, err := store.Exists(filepath)
	if !trainingSetExists {
		return nil, &TrainingSetNotFound{id}
	}
	if err != nil {
		return nil, fmt.Errorf("failed to check if training set exists: %w", err)
	}
	files, err := store.List(filepath, filestore.Parquet)
	if err != nil {
		return nil, fmt.Errorf("could not get training set: %v", err)
	}
	groups, err := filestore.NewFilePathGroup(files, filestore.DateTimeDirectoryGrouping)
	if err != nil {
		return nil, fmt.Errorf("could not group files by datetime directory: %v", err)
	}
	newestFiles, err := groups.GetFirst()
	if err != nil {
		return nil, fmt.Errorf("could not get newest files: %v", err)
	}
	iterator, err := store.Serve(newestFiles)
	if err != nil {
		return nil, fmt.Errorf("could not serve training set: %w", err)
	}
	return &FileStoreTrainingSet{id: id, store: store, iter: iterator}, nil
}

type FileStoreTrainingSet struct {
	id       ResourceID
	store    FileStore
	iter     Iterator
	Error    error
	features []interface{}
	label    interface{}
}

func (ts *FileStoreTrainingSet) Next() bool {
	row, err := ts.iter.Next()
	if err != nil {
		ts.Error = err
		return false
	}
	if row == nil {
		return false
	}
	featureValues := make([]interface{}, len(ts.iter.FeatureColumns()))
	for i, key := range ts.iter.FeatureColumns() {
		featureValues[i] = row[key]
	}
	ts.features = featureValues
	ts.label = row[ts.iter.LabelColumn()]
	return true
}

func (ts *FileStoreTrainingSet) Features() []interface{} {
	return ts.features
}

func (ts *FileStoreTrainingSet) Label() interface{} {
	return ts.label
}

func (ts *FileStoreTrainingSet) Err() error {
	return ts.Error
}
